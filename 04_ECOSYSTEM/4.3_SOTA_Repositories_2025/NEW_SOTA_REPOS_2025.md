# üöÄ Novos Reposit√≥rios Estado-da-Arte (Outubro 2025)

**Data de Descoberta:** 30 de Outubro de 2025  
**Fonte:** An√°lise de arquivo externo com 7.000+ reposit√≥rios  
**Total de Novos Reposit√≥rios √önicos:** 35

Este documento lista os reposit√≥rios de IA estado-da-arte que foram validados e considerados adi√ß√µes √∫nicas ao `AIForge`.

---

| # | Reposit√≥rio | Stars | Linguagem | Descri√ß√£o |
|---|---|---|---|---|
| 1 | [mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech) | 26638 ‚≠ê | C++ | DeepSpeech is an open source embedded (offline, on-device) speech-to-text engine which can run in real time on devices ranging from a Raspberry Pi 4 to high power GPU servers. |
| 2 | [Stability-AI/generative-models](https://github.com/Stability-AI/generative-models) | 26550 ‚≠ê | Python | Generative Models by Stability AI |
| 3 | [Stability-AI/generative-models](https://github.com/Stability-AI/generative-models) | 26550 ‚≠ê | Python | Generative Models by Stability AI |
| 4 | [microsoft/BitNet](https://github.com/microsoft/BitNet) | 24321 ‚≠ê | Python | Official inference framework for 1-bit LLMs |
| 5 | [Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet) | 17833 ‚≠ê | Python | WebUI extension for ControlNet |
| 6 | [state-spaces/mamba](https://github.com/state-spaces/mamba) | 16261 ‚≠ê | Python | Mamba SSM architecture |
| 7 | [BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) | 14078 ‚≠ê | Python | RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 "Goose". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding. |
| 8 | [IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) | 9160 ‚≠ê | Python | [ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\" |
| 9 | [ashawkey/stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion) | 8753 ‚≠ê | Python | Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion. |
| 10 | [mit-han-lab/streaming-llm](https://github.com/mit-han-lab/streaming-llm) | 7097 ‚≠ê | Python | [ICLR 2024] Efficient Streaming Language Models with Attention Sinks |
| 11 | [InternLM/InternLM](https://github.com/InternLM/InternLM) | 7091 ‚≠ê | Python | Official release of InternLM series (InternLM, InternLM2, InternLM2.5, InternLM3). |
| 12 | [zai-org/GLM-4](https://github.com/zai-org/GLM-4) | 6911 ‚≠ê | Python | GLM-4 series: Open Multilingual Multimodal Chat LMs - ÂºÄÊ∫êÂ§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÂØπËØùÊ®°Âûã |
| 13 | [threestudio-project/threestudio](https://github.com/threestudio-project/threestudio) | 6903 ‚≠ê | Jupyter Notebook | A unified framework for 3D content generation. |
| 14 | [lucidrains/x-transformers](https://github.com/lucidrains/x-transformers) | 5645 ‚≠ê | Python | A concise but complete full-attention transformer with a set of promising experimental features from various papers |
| 15 | [kyegomez/swarms](https://github.com/kyegomez/swarms) | 5350 ‚≠ê | Python | The Enterprise-Grade Production-Ready Multi-Agent Orchestration Framework. Website: https://swarms.ai |
| 16 | [facebookresearch/sapiens](https://github.com/facebookresearch/sapiens) | 5189 ‚≠ê | Python | High-resolution models for human tasks. |
| 17 | [deepseek-ai/DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2) | 4953 ‚≠ê | N/A | DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model |
| 18 | [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro) | 4944 ‚≠ê | Python | Depth Pro: Sharp Monocular Metric Depth in Less Than a Second. |
| 19 | [NExT-GPT/NExT-GPT](https://github.com/NExT-GPT/NExT-GPT) | 3576 ‚≠ê | Python | Code and models for ICML 2024 paper, NExT-GPT: Any-to-Any Multimodal Large Language Model |
| 20 | [microsoft/PhiCookBook](https://github.com/microsoft/PhiCookBook) | 3564 ‚≠ê | Jupyter Notebook | This is a Phi Family of SLMs book for getting started with Phi Models. Phi a family of open sourced AI models developed by Microsoft. Phi models are the most capable and cost-effective small language models (SLMs) available, outperforming models of the same size and next size up across a variety of language, reasoning, coding, and math benchmarks |
| 21 | [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | 2942 ‚≠ê | Python | ‚ö°LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.‚ö° |
| 22 | [databricks/dbrx](https://github.com/databricks/dbrx) | 2573 ‚≠ê | Python | Code examples and resources for DBRX, a large language model developed by Databricks |
| 23 | [zai-org/CogVLM2](https://github.com/zai-org/CogVLM2) | 2420 ‚≠ê | Python | GPT4V-level open-source multi-modal model based on Llama3-8B |
| 24 | [InternLM/InternLM-XComposer2](https://github.com/InternLM/InternLM-XComposer2) | 2045 ‚≠ê | Python | InternLM-XComposer2: A Vision-Language Large Model for Advanced Text-Image Comprehension and Generation |
| 25 | [bigcode-project/starcoder2](https://github.com/bigcode-project/starcoder2) | 1981 ‚≠ê | Python | Home of StarCoder2! |
| 26 | [PixArt-alpha/PixArt-sigma](https://github.com/PixArt-alpha/PixArt-sigma) | 1857 ‚≠ê | Python | PixArt-Œ£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation |
| 27 | [google-research/robotics_transformer](https://github.com/google-research/robotics_transformer) | 1617 ‚≠ê | Python | N/A |
| 28 | [octo-models/octo](https://github.com/octo-models/octo) | 1403 ‚≠ê | Python | Octo is a transformer-based robot policy trained on a diverse mix of 800k robot trajectories. |
| 29 | [microsoft/ToRA](https://github.com/microsoft/ToRA) | 1101 ‚≠ê | Python | ToRA is a series of Tool-integrated Reasoning LLM Agents designed to solve challenging mathematical reasoning problems by interacting with tools [ICLR'24]. |
| 30 | [SCIR-HI/Med-ChatGLM](https://github.com/SCIR-HI/Med-ChatGLM) | 1022 ‚≠ê | Python | Repo for Chinese Medical ChatGLM Âü∫‰∫é‰∏≠ÊñáÂåªÂ≠¶Áü•ËØÜÁöÑChatGLMÊåá‰ª§ÂæÆË∞É |
| 31 | [huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto) | 1001 ‚≠ê | Python | A pytorch quantization backend for optimum |
| 32 | [IST-DASLab/sparsegpt](https://github.com/IST-DASLab/sparsegpt) | 844 ‚≠ê | Python | Code for the ICML 2023 paper "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot". |
| 33 | [kyegomez/zeta](https://github.com/kyegomez/zeta) | 558 ‚≠ê | Python | Build high-performance AI models with modular building blocks |
| 34 | [01-ai/Yi-1.5](https://github.com/01-ai/Yi-1.5) | 556 ‚≠ê | N/A | Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability. |
| 35 | [Stability-AI/StableCode](https://github.com/Stability-AI/StableCode) | 125 ‚≠ê | Jupyter Notebook | Code Assistance/ Developer Productivity suite of Models  |
