# Humans vs AI - Benchmarks e RepositÃ³rios (2025)

**Data:** 31 de Outubro de 2025

---

## ğŸ“Š Resumo

Esta seÃ§Ã£o compila repositÃ³rios e benchmarks focados na comparaÃ§Ã£o direta de performance entre humanos e inteligÃªncia artificial.

---

## ğŸš€ RepositÃ³rios GitHub

| # | RepositÃ³rio | Stars | DescriÃ§Ã£o |
|---|---|---|---|
| 1 | [**METR/ai-rd-tasks**](https://github.com/METR/ai-rd-tasks) | 114â­ | Tarefas de R&D de IA usadas no benchmark RE-Bench. |
| 2 | [**philschmid/ai-agent-benchmark-compendium**](https://github.com/philschmid/ai-agent-benchmark-compendium) | 47â­ | CompÃªndio de 50+ benchmarks para avaliar AI agents. |
| 3 | [**aflah02/Humans-v-s-LLM-Benchmarks**](https://github.com/aflah02/Humans-v-s-LLM-Benchmarks) | 1â­ | Ferramenta interativa de quiz baseada em benchmarks LLM. |
| 4 | [**vslaykovsky/ai-sourcing-benchmark**](https://github.com/vslaykovsky/ai-sourcing-benchmark) | 1â­ | Benchmark de sourcing. |
| 5 | [**mikosa01/AI_VS_Human**](https://github.com/mikosa01/AI_VS_Human) | 0â­ | ComparaÃ§Ã£o IA vs Humanos. |

---

## ğŸ¯ Benchmarks NotÃ¡veis

### RE-Bench
- **Desenvolvedor:** METR (Monitoring and Evaluation of AI for Emerging Risks)
- **Foco:** Comparar AI agents com especialistas humanos em tarefas de R&D de IA.
- **Resultado:** AI agents pontuam 4X mais que humanos com orÃ§amento de 2 horas.

### SWE-Bench Pro
- **Foco:** Engenharia de software.
- **Resultado:** AI agents ~23% vs humanos melhorando com experiÃªncia.

### HumanEval
- **Foco:** 164 desafios de programaÃ§Ã£o.
- **Objetivo:** Testar modelos contra performance de nÃ­vel humano.

### Cybersecurity CTF
- **Foco:** CompetiÃ§Ãµes de Capture The Flag em ciberseguranÃ§a.
- **Resultado:** IA supera humanos com 95% de taxa de sucesso.

---

[â†©ï¸ Voltar para o README principal](../../README.md)
