# Humans vs AI - Benchmarks e Repositórios (2025)

**Data:** 31 de Outubro de 2025

---

## 📊 Resumo

Esta seção compila repositórios e benchmarks focados na comparação direta de performance entre humanos e inteligência artificial.

---

## 🚀 Repositórios GitHub

| # | Repositório | Stars | Descrição |
|---|---|---|---|
| 1 | [**METR/ai-rd-tasks**](https://github.com/METR/ai-rd-tasks) | 114⭐ | Tarefas de R&D de IA usadas no benchmark RE-Bench. |
| 2 | [**philschmid/ai-agent-benchmark-compendium**](https://github.com/philschmid/ai-agent-benchmark-compendium) | 47⭐ | Compêndio de 50+ benchmarks para avaliar AI agents. |
| 3 | [**aflah02/Humans-v-s-LLM-Benchmarks**](https://github.com/aflah02/Humans-v-s-LLM-Benchmarks) | 1⭐ | Ferramenta interativa de quiz baseada em benchmarks LLM. |
| 4 | [**vslaykovsky/ai-sourcing-benchmark**](https://github.com/vslaykovsky/ai-sourcing-benchmark) | 1⭐ | Benchmark de sourcing. |
| 5 | [**mikosa01/AI_VS_Human**](https://github.com/mikosa01/AI_VS_Human) | 0⭐ | Comparação IA vs Humanos. |

---

## 🎯 Benchmarks Notáveis

### RE-Bench
- **Desenvolvedor:** METR (Monitoring and Evaluation of AI for Emerging Risks)
- **Foco:** Comparar AI agents com especialistas humanos em tarefas de R&D de IA.
- **Resultado:** AI agents pontuam 4X mais que humanos com orçamento de 2 horas.

### SWE-Bench Pro
- **Foco:** Engenharia de software.
- **Resultado:** AI agents ~23% vs humanos melhorando com experiência.

### HumanEval
- **Foco:** 164 desafios de programação.
- **Objetivo:** Testar modelos contra performance de nível humano.

### Cybersecurity CTF
- **Foco:** Competições de Capture The Flag em cibersegurança.
- **Resultado:** IA supera humanos com 95% de taxa de sucesso.

---

[↩️ Voltar para o README principal](../../README.md)
